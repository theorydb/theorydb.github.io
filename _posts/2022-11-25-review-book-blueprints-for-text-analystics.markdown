---  
layout: post  
title: "[리뷰] 파이썬 라이브러리를 활용한 텍스트 분석"  
subtitle: "텍스트에서 통찰을 이끌어내는 98가지 자연어 처리 전략"  
categories: review  
tags: review book 텍스트 의미론 NLP EDA 통계 크롤링 API 전처리 머신러닝 LDA NMF SVM 라이브러리 분류 감성분석 지식그래프 배포 프로덕션     
comments: true  
header-img: img/review/review-book-blueprints-for-text-analystics-1.png
---  
  
> `한빛미디어` 출판사의 `"파이썬 라이브러리를 활용한 텍스트 분석(젠스 알브레히트, 싯다르트 라마찬드란, 크리스티안 윙클러 저/심상진 역)"`를 읽고 작성한 리뷰입니다.  

![표지](https://theorydb.github.io/assets/img/review/review-book-blueprints-for-text-analystics-1.png)  

---

> 텍스트 분석에 필요한 거의 모든 기술과 라이브러리를 핵심만 쉽고 간결하게 전달하고 있어 놀랍다.

이 책의 가장 큰 장점은 `생산성`에 있는 것 같다. 독자로 하여금 빠른 시간 내 텍스트 분석 실무에 적응할 수 있고 한 권의 책으로 제품화까지 구상하는 데 큰 무리가 없게 구성하고 있기 때문이다. 

책의 원서 제목이 "Blueprints for Text Analytics using Python"인데 가히 `Blueprints`라는 용어를 쓸 수 있을만큼 텍스트 분석 생태계에 필요한 전반을 한 권의 책으로 통일성있고 유기성있게 엮어냈다는 것이 신기하다.

이러한 실용적인 생산성을 높여주는 근거는 크게 두 부분으로 나눌 수 있겠다.

하나는 텍스트 생태계의 `Python 라이브러리`를 일목요연하게 훑어보고 바로 실전에 적용할 수 있도록 구성되었다는 점을 들 수 있고, 다른 하나로 텍스트 분석 전반에 필요한 `기술`을 대부분 아우르고 있다는 점을 들 수 있겠다.

먼저 텍스트 데이터를 둘러싼 Python 진영의 Pandas와 같은 기본 라이브러리에서 부터 Gensim과 같은 비교적 최신의 임베딩 라이브러리에 이르기까지 왠만한 현업 종사자도 전부 다 활용해 보진 못했을거라 추측될 정도로 방대한 라이브러리를 예제에 담아냈다.

아울러 기본적인 통계지식을 활용하여 빠르게 EDA로 데이터에 친숙해지며 감을 잡는 것을 시작으로 API나 크롤링을 통해 외부 데이터 세계를 가져와 전처리 하는 방법, 또 이를 전처리하여 활용할 수 있는 형태로 가공하는 방법, 준비된 데이터를 머신러닝에 적용해보며 토픽 모델링이나 분류 작업을 학습할 수 있음은 물론 책의 뒷부분으로 갈수록 지식그래프와 같은 시멘틱 기술을 활용하여 머신러닝의 단점을 보완할 수 있게 해주고 도커나 콘다를 이용한 배포 및 제품화까지 고려하고 있다.

조금 더 자세히 살펴보겠다. 1장에서는 주어진 데이터에 `친숙`해지는데 있어 가장 효율적이고 빠른 방법을 소개하고 있다. 

그동안 실무에서 텍스트 분석에 임하다보면 꼬리에 꼬리를 무는 생각 때문에 시간을 많이 소요하는 일이 잦았다. 가능할지 불가능할지 혹은 주어진 데이터로 할 수 있는 일인지 조차 생각하지 못한채 거창한 아이디어를 꿈꾸다가 시간을 낭비하는 일이 종종 있는데 언제나 돌이켜보면 어느 데이터에나 적용할 수 있는 심플하고 빠른 방법을 먼저 수행하는 편이 좋았다는 생각이 들었고 나름의 정형화된 방법을 정리해 적용해오고 있었는데 1장에 제시된 방법들은 적어도 몇년 간 내가 정리한 방법보다 깔끔하고 빠르게 분석할 수 있었기에 느낀 바가 컸다.

아울러 1장에서는 데이터에 친숙해지기 위한 `EDA` 과정외에도 불용화나 토큰화와 같은 기본적인 `전처리` 방법도 배운다. 

2장 ~ 3장은 `API나 크롤링`을 활용하여 데이터를 `수집`하는 노하우를 담은 장이다. 역시 짧은 지면에 데이터 수집과 관련된 시행착오를 쉽게 해결할 수 있는 방법을 잘 요약하고 있다. 의외로 데이터 분석가 중에 REST API를 호출하며 HTTP의 기본 Response Code를 해석하지 못해 쩔쩔매는 경우를 흔히 봐왔는데 친절히 코드별 대응책을 알려주고 있어 인상적이었다.

깃허브의 API, 트위피, 스크래핑 등의 예제를 따라할 수 있는 것 자체로도 좋은 예제 구성이지만 robot.txt를 시작으로 과부하로 부터 서버를 보호하기 위해 서버측에서 행하는 제약들을 유연성있게 피하며 데이터를 수집할 수 있는 좋은 팁들이 더 값지다는 생각이 든다.
![트위피](https://theorydb.github.io/assets/img/review/review-book-blueprints-for-text-analystics-2.png)  

4장은 앞서 수집한 데이터를 `가공`하는데 초점을 맞춘다. 표준화 작업을 시작으로 팀 내부 간 쉽고 빠른 공유를 위해 SQLite를 활용하여 데이터를 축적하는 방법을 다루고 있으며 노이즈 제거, 토큰화, 원형추출 등 보다 심화된 전처리 방법을 다루고 있는데 이 역시 현업에서 가장 중요한 부분의 핵심만 담아내고 있어 빠른 시간 내 실무 능력을 키우는데 큰 도움이 되리라 생각한다. 

5장 ~ 8장은 본격적인 `머신러닝` 알고리즘을 적용하는 장이다. 5장에서 머신러닝에 필요한 기본 개념들 즉, 벡터화, TF-IDF, 차원축소, 구문유사성과 관련된 기본기를 다진 후 6장에서 SVM 분류 알고리즘을 적용하며 기본적인 머신러닝 활용법을 정리해 볼 수 있다.

다른 책들과 차별화 된 부분이 주로 7장 ~ 8장에서 많이 실려있는데 기본기를 익히는 데 한 걸음 더 나아가서 `피처 엔지니어링이나 XAI` 측면에 집중하며 분석가로 하여금 더 나은 아이디어를 떠올리게 함은 물론 뒤에 이어질 고차원 NLP처리를 위한 탄탄한 기본기를 잡는데 도움을 준다. 

예를 들면 7장에는 `해석`을 위한 여러 시각화 도구를 활용하는데 LIME과 같이 전통적으로 자주 활용하는 라이브러리도 등장하지만 개인적으로 ELI5나 앵커와 같이 자주 사용해보지 않은 라이브러리가 자세히 소개되고 있어 많은 도움을 받을 수 있었다. 
![ELI5](https://theorydb.github.io/assets/img/review/review-book-blueprints-for-text-analystics-3.png)  

특히 pyLDAvis와 같이 한 번도 활용해보지 못한 라이브러리도 다수 만날 수 있었는데 빠른 시간 내에 활용할 수 있게 구성되어있어 인상적이었다. 뭐든 있는지, 없는지 자체라도 알면 쉽게 학습할 수 있는데 있는지, 없는지 자체를 모르면 빙빙 돌아가고 시간을 크게 낭비하게되니 말이다.
![pyLDAvis](https://theorydb.github.io/assets/img/review/review-book-blueprints-for-text-analystics-4.png)  

아무튼 7장 ~ 8장을 거치며 `feature importance`를 중심으로 데이터 모델에 대한 해석력을 확보할 수 있는 과정은 매우 유익했고, 이를 통해 토픽모델링을 실습하며 뒤에 이어질 고급 기술들의 이해력을 높이는 구성이 매우 마음에 들었다. 

더불어 NMF, LDA 등의 알고리즘을 실습해보며 선형대수 등의 기초 학문 지식이 어떻게 기술에 응용되는지 볼 기회도 제공되고 있어 `학문과 산업 간 연계 이해` 측면에도 도움이 될 것 같다.

9장 이후에는 보다 실용적인 관점으로 책의 집필 방향이 변하는 것 같다. 사실 텍스트 요약과 같은 주제는 능히 책 한 권을 할당하고도 모자를 만큼 방대한 주제이기에 이에 대한 밑바닥을 살피는 것은 쉽지 않으므로 라이브러리를 `활용`하여 빠르게 원하는 답을 찾는 수준으로만 소개되어있다. 

임베딩이나 의미론적 유사성을 찾는 과정 역시 Gensim과 같은 라이브러리 활용에만 초점을 맞추고 있다. 실무에서 그 내부를 들여다 볼 필요는 크게 없기에 이 자체로도 유용하다고 생각하며 앞서 언급했듯 7 ~ 8장에서 얻은 지식으로 내부를 충분히 상상하고 이해하는데 무리가 없을 것이다.

번외로 R에 비해 Python이 마음에 안 드는 대표적인 이유를 꼽자면 시각화 정도를 들 수 있겠는데 특히 Matplotlib이 개인적으로 가장 마음에 들지 않았다. 이 역시 Matplotlib만 고집하지 않고 Plotly와 같은 훨씬 좋은 라이브러리 등 예제마다 적재적소에 필요한 라이브러리를 잘 선택하며 예제를 구성하고 있어 마음에 들었다.
![Plotly](https://theorydb.github.io/assets/img/review/review-book-blueprints-for-text-analystics-5.png)  

아무튼 앞서 배웠던 지식들을 토대로 11장에서는 감성 분석 예제로 그간의 지식을 잘 `정리`할 수 있게 도와준다. 

12장이나 13장은 약간 부록의 성격에 가깝다. 12장은 지식그래프 구축 주제를 다루는 데 이는 `시멘틱 기술` 진영의 이야기이다. 자동화된 머신러닝 진영에 비해 아래 그림과 같이 사람이 지정하는 규칙에 큰 영향을 받는다. 
![지식그래프](https://theorydb.github.io/assets/img/review/review-book-blueprints-for-text-analystics-6.png)  

양 진영의 정반합이 텍스트 분석 생태계의 발전에 큰 역할을 하고 있으므로 비교 우위를 논하는 것은 별 의미가 없으며 시멘틱 기술에 관심이 있다면 최근에 작성한 관련 책 리뷰를 참고하기 바란다. [리뷰 - 시맨틱 데이터 모형화](https://theorydb.github.io/review/2022/10/29/review-book-semantic-modeling/)

13장은 콘다와 YAML로 개발 환경을 구성하는 방법이나 도커 환경을 구성하는 방법, 심지어 WSGI서버를 활용하여 API 서버를 구축하고 빌드 및 배포를 자동화하는 방법을 다루고 있다. `제품화`와 관련된 핵심 기술을 단 하나의 챕터로 핵심만 간결하게 전달하고 있다는 점이 매우 놀라웠다. 

이 부분에서 큰 그림을 잡지 못하면 좋은 모델을 갖고도 제품이나 서비스는 산으로 가는 경우가 많은데 경험이 풍부한 전문가의 전달력으로 짧은 지면에 핵심을 담아내는 능력에 감탄했다.

전반적으로 텍스트 분석에 있어 필요한 `A to Z`를 매우 빠르게 습득할 수 있는 책이다. 그렇기에 생산성 측면 즉, 공부한 시간 대비 가장 많은 기술과 지식을 익힐 수 있는 책이라 생각하며 빠르게 현업 및 실무에 배운 기술을 써먹을 수 있도록 안배된 책이라 생각한다.

아마도 BERT나 GPT-3와 같은 딥러닝 모델을 제외하고는 적어도 내가 아는 모든 기술이 `콤팩트`하게 담겨있다고 본다.

다만, 모든 책에는 독자의 `수준`이 중요한 것 같다. 너무 좋은 책인데도 독자가 극도로 초보자이거나 전문가여서 비판 받는 경우를 많이 봐왔다. 그렇기에 이 책 역시 독자 스스로의 수준 파악이 중요할 것 같다.

Python의 기본 문법은 물론 기본 생태계 정도는 충분히 활용해보고, 분석 계열이 아닌 IT 분야에서라도 문자열 처리 (특히, 정규표현식) 정도는 무수히 다뤄봤으며, 기본적인 머신러닝 알고리즘 혹은 통계 지식 및 선형대수 정도의 지식을 갖추고 있다면 2, 3, 13장을 제외하고는 무난한 이해가 가능하리라 생각한다.

다만 2, 3, 13장은 기본적인 IT 지식이 필요한 장이고 특히 13장은 하나하나의 주제별로 책 한 권씩 낼 수 있는 부분이라 각 챕터를 지도삼아 다른 자료를 많이 참조하면 좋을 듯 하다. 

텍스트 분석에 입문한다면 가장 처음으로 볼만한 책으로 추천하고 싶다. 이 책이 기준점이 된다면 보다 고차원 적인 기술이나 모델을 익히는데 넘어야 할 장애물을 최소화할 수 있을 것 같다는 생각이 들며 이 분야의 전체 지도를 머릿속에 그리고 출발할 수 있다는 것이 전문가로 성장하는데 큰 도움을 줄 수 있을거라 생각한다.

---

* [책소개 - 파이썬 라이브러리를 활용한 텍스트 분석](http://www.yes24.com/Product/Goods/114292949)
